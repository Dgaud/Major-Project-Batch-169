import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from sklearn1 import mean_squared_error, mean_absolute_error, r2_score, score
import pandas as pd

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('/content/modified_dataset.csv')

# Convert negative values in the 'weight' column to their absolute values
df['weight'] = df['weight'].abs()
df['feed_intake']=df['feed_intake'].abs()

# Save the modified DataFrame back to a CSV file
df.to_csv('modified_dataset.csv', index=False)
# Load sensor data from CSV file
data = pd.read_csv("/content/modified_dataset.csv")

# Handle missing values (e.g., imputation)
data.fillna(value=0, inplace=True)

data

# Convert the "timestamp" column to a numeric data type
data["timestamp"] = pd.to_datetime(data["timestamp"])

# Create new features (e.g., rolling averages)
data["temp_mean_7d"] = data["temperature"].rolling(window=7).mean()
data.fillna(value=0, inplace=True)
# One-hot encode the "timestamp" column
encoder = OneHotEncoder(handle_unknown="ignore")
data_encoded = encoder.fit_transform(data[["timestamp"]])

# Combine the encoded features with the other features
data_for_selection = pd.concat([data.drop(["timestamp", "target_column"], axis=1), pd.DataFrame(data_encoded.toarray())], axis=1)

# Ensure the target column is categorical or discrete
data["target_column"] = data["target_column"]
# Convert all column names in 'data_for_selection' to strings
data_for_selection.columns = data_for_selection.columns.astype(str)

data_for_selection.fillna(value=0, inplace=True)

# Now perform feature selection using chi-square test
chi2_selector = SelectKBest(chi2, k=10)  # Select top k features
chi2_selector.fit(data_for_selection, data["target_column"])

# Get selected feature indices
selected_features_indices = chi2_selector.get_support(indices=True)

# Print the selected features
print("Selected features indices:", selected_features_indices)
  # Ensure that the indices are within the range of DataFrame's columns
valid_indices = [i for i in selected_features_indices if i < len(data.columns)]

# Get the column names for the valid selected features
selected_feature_names = data.columns[valid_indices]

# Exclude the 'target_column' from the features
selected_feature_names = selected_feature_names[selected_feature_names != 'target_column']

# Split data into training and testing sets
X = data[selected_feature_names]
y = data["target_column"]
X_train, X_test, y_train, y_test = train_test_split(X, y)  # Added test_size and random_state for reproducibility

# Train a Random Forest model
model = RandomForestRegressor()
model.fit(X_train, y_train)

predictions = [1.1, 2.2, 3.3]
y_test = [1, 2, 3]
                 # @title Target Column vs Air Quality and Temperature

import matplotlib.pyplot as plt
_ = plt.scatter(data['air_quality'], data['temperature'], c=data['target_column'])
                 import matplotlib.pyplot as plt
plt.scatter(data['timestamp'], data['temperature'], c=data['humidity'])
plt.xlabel('timestamp')
_ = plt.ylabel('temperature')

                 # Calculate model accuracy
accuracy = score(y_test, predictions)
print("Accuracy:", accuracy)

# Calculate mean squared error (MSE)
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculate mean absolute error (MAE)
mae = mean_absolute_error(y_test, predictions)
print("Mean Absolute Error:", mae)

# Calculate R-squared (R2) score
r2 = r2_score(y_test, predictions)
print("R-squared Score:", r2)
